{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 6350 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your info\n",
    "* NetID(s): tz332\n",
    "* Name(s): Tammy Zhang\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# needed installations\n",
    "# pip install praw\n",
    "# pip install geonamescache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import geonamescache\n",
    "import spacy\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What problem are you working on? Why is it interesting and important? What have other people said about it? What do you expect to find?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A screenshot of the visualization upon launch](home.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can NLP approaches and tools such as Spacy be potentially used in developing informative and engaging interactive visualizations for text-based data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My research problem is investigating how computational text tools such as Spacy might be used to help supplement interactive web visualization pipelines. More specifically, I experiment with incorporating Spacy's entity recognition abilities into a visualization development workflow for exploring answers to five similar US-state-based questions posed on [r/AskReddit](https://www.reddit.com/r/AskReddit/), a popular community on the social media platform Reddit. \n",
    "\n",
    "My contribution to this space includes the following 1.) an example simple web application that draws on data generated through text processing including Spacy's entity recognition to visualize text data on a U.S. state map for 5 popular r/AskReddit questions; 2.) discussion on possible future directions for using NLP methods in text data visualization based on what I learned along the way while creating a custom workflow, and 3.) the specifications for a set of Python functions that together can take a URL to a Reddit post with thousands of comments and transform it into a `.json` file compatible with open-source, lightweight Javascript libraries such as [d3.js](https://d3js.org/) to rapidly generate interactive map visualizations shareable on the web with minimal hosting needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing text data is noted as a [particularly challenging task in data visualization courses](https://hci.stanford.edu/courses/cs448b/f12/lectures/CS448B-20121115-Text.pdf) compared to traditional quantitative data, primarily because of the curse of dimensionality and the risk of producing results with low interpretability. However, much data on today's Internet is primarily in text content - particularly much user-generated content - which may carry important information on current social/economic/emotional landscapes and trends, but run the risk of being completely buried, lost, and overlooked due to the sheer quantity of text that is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such example of a source of user-generated texts is Reddit, an American social media platform that is currently [one of the most visited websites in the world](https://www.sci-tech-today.com/stats/reddit-statistics/). There are currently more than 500 million Reddit users, expected to increase to 556 million in the next few years. The website includes over 100000 subreddits, which form communities of common interest where users can gather to post text, images, and other content regarding particular topics; users often add their own replies under others' posts and to other people's comments as well. Of these subreddits, the second most popular is r/AskReddit with over 50 million members, where users post questions for others to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These questions are frequently of a humorous, rhetorical, and opinion-based nature - but many may also be serious and telling of more widescale social attitudes, movements, and trends. Humanities researchers can potentially discover emerging themes and patterns of interest in investigating the responses of thousands of people to questions such as \"What keeps you going in difficult times?\" \"What are you worried about right now?\" \"What is a personal experience you've had with __ ?\" However, individual replies to popular posts frequently number in the thousands, and comments are frequently buried under multiple levels of nested replies or algorithmic sorting that prioritizes \"hot\" comments with more upvotes. Thus, extracting meaningful information or themes from this information may be difficult. Visualization is often used to get broad-level insights about general trends in a body of data, but generating effective visualizations generally requires structured data - again, something challenging to extract from thousands of text comments, even if Reddit's particular structure around subreddits might somewhat help concentrate text around specific topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, my project aims to complete a preliminary exploration of how someone might begin to bring information buried across thousands of comments up into a visible interface. I do this by exploring a particular category of question on r/AskReddit - those related to American states - to see if very simple NLP methods might add value into data visualization workflows and interface designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My motivation for selecting Reddit questions related to American states in particular is because it poses a relatively simple task that can be addressed to some extent using entity recognition, and because the expected visual representation of the information is fairly standard - in the form of a map. By focusing on questions with a simpler, more closed set of possible answers, I can experiment with my workflow instead of overly concerning myself on the most intuitive ways of representing patterns in user responses. I discuss in Future Work several directions in which I can see NLP playing more advanced roles and with different visualization forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This direction was also informed in part by my own research background in and current work in the field of interactive data visualization. After building a number of web-based visualizations of different datasets, many multidimensional and/or geospatial, I've found that there is an overwhelming amount of tools and strategies available when choosing the best approach to data processing and interaction design planning each time. However, I have not yet tried to use any computational text processing in my own work, and I also have not attempted to make complex interactive visualizations of primarily text-based data in the past, so I felt that this project would be a good opportunity for me to begin experimenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of users have attempted to create data visualizations for Reddit in the past, but most have focused on website-level visualization of connections between overall subreddits based on content similarity or shared user activity ([1](https://www.reddit.com/r/dataisbeautiful/comments/mfmlho/oc_ive_made_an_interactive_map_of_reddit_based_on/), [2](https://redditmap.social/)). After searching through several visualization-related subreddits, I found only a few user-created posts attempting to visualize answers to r/AskReddit questions: [What is one country that you will never visit again?](https://www.reddit.com/r/dataisbeautiful/comments/omr3x4/comment/h5mqqi0/) and [The most hated U.S. states, according to r/AskReddit](https://www.reddit.com/r/dataisbeautiful/comments/p3yy6o/oc_the_most_hated_us_states_according_to/). Both posts were quite relatively popular, but both graphics are static and neither user explicitly mentioned using any particular text processing methods in their workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also searched for scholarly works related to r/AskReddit; I found that most papers tended to be high-level (focused on differences/relationships between communities) or more atomic (focused on conversation-level signals, trends, and interpersonal interaction). I was not able to locate any papers focused on using computational text techniques to cluster comments on a _post level_ to identify emerging themes in one particular context (i.e. one instance of a question posed by a user), or for use in data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally, I became interested in Reddit as a data source partly after reading [Antoniak et. al (2019)](https://maria-antoniak.github.io/resources/2019_cscw_birth_stories.pdf), in which birth stories, which historically have not been focused on by researchers, gained visibility through the use of Reddit in sharing one's personal narrative. I was also interested in the intersection of geographic visualization through a map and text analysis in [Wilkens et. al (2024)](https://tuprints.ulb.tu-darmstadt.de/27523/1/3917_Small_Worlds_Conference_Version.pdf). The inclusion of a map in Figure 2 led me to consider my past experiences collecting and cleaning data for visualization in maps, and also led me to the beginning of questioning how text mining might integrate with complex data visualization - moving beyond simple count-based graphics like word clouds or lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect that computational methods such as using Spacy's entity recognition tools will be helpful in 1.) extracting and aggregating more information from Reddit comments than would be feasible on a more manual level and 2.) helping to standardize a workflow so that once a kind of visualization can be generated for a particular post's comments, it will be relatively quick and costless to generate visualizations for posts similar in structure/topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I also expect that without further processing or the inclusion of more complex text processing methods than simply extracting data points of interest, the upfront value of the increased volume of information may be limited - primarily because it might be difficult to meaningfully interpret. Since people on the Internet frequently may use sarcasm, informal language, inside jokes, and other difficult-to-detect tonal indicators that change their intended meaning, using such a blunt method that checks primarily for the presence of an entity is likely to produce at least sometimes confusing or inconsistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What data have you used? Where did it come from? How did you collect it? What are its limitations or omissions? What major methods will you use to analyze it? Why are those methods the appropriate ones?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Source and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data consists of comments under five r/AskReddit questions, which each ask users to name particular U.S. states in their answers. Initially, I worked on developing a pipeline for creating a map-based visualization for the first question, with the smallest number of comments; once I had a working graphic for this example, I applied the same workflow to the rest of the questions. Each question's replies were fetched, processed, and visualized separately. For the first question, the length of each comment ranged from just two characters long to 4453 characters long.\n",
    "\n",
    "1. [What do you think is the best U.S. state?](https://www.reddit.com/r/AskReddit/comments/172l3yj/what_do_you_think_is_the_best_us_state/) (4105 replies)\n",
    "\n",
    "2. [What’s the one US state you absolutely will never step foot in and why?](https://www.reddit.com/r/AskReddit/comments/1bfq75y/whats_the_one_us_state_you_absolutely_will_never/) (10864 replies)\n",
    "\n",
    "3. [What is perhaps the least talked about US state?](https://www.reddit.com/r/AskReddit/comments/13mat7e/what_is_perhaps_the_least_talked_about_us_state/) (5580 replies)\n",
    "\n",
    "4. [US redditors, what does your state do better than all the others?](https://www.reddit.com/r/AskReddit/comments/4gjd4h/us_redditors_what_does_your_state_do_better_than/) (7775 replies)\n",
    "\n",
    "5. [All 50 states are getting together for Thanksgiving dinner. What does your state bring and why?](https://www.reddit.com/r/AskReddit/comments/3uahca/all_50_states_are_getting_together_for/) (7225 replies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected these particular posts by applying the search term \"state\" to r/AskReddit and then sorting by Top of All Time. My motivation in doing this was to select groups of texts that would be somewhat similar in their manner of addressing the question, but would vary in meaning, and also to ensure that I would have sufficient comments to gather substantive data from per post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collected this data using the [Python Reddit API Wrapper (PRAW) library](https://praw.readthedocs.io/en/stable/). I consulted the documentation on how to get started with registering an application on Reddit and authenticating through OAuth so that my requests would follow Reddit policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initializing an authorized Reddit Instance for scraping comments\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"redacted\",\n",
    "    client_secret=\"redacted\",\n",
    "    password=\"redacted\",\n",
    "    user_agent=\"mac:6350-project:v1.0 (by /u/redacted)\",\n",
    "    username=\"redacted\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up my Reddit authentication, I wrote two functions to help properly aggregate comments under a given post. `get_top_level_comments` only collected top level comments (replies directly to the post itself), and `get_all_comments` collected all comments under a post, including replies to other comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_top_level_comments(post_url):\n",
    "    \"\"\"\n",
    "    A function for aggregating top-level comment responses to a Reddit post into a pandas dataframe.\n",
    "    \n",
    "    Parameter post_url: the url link to the intended Reddit post. Type string.\n",
    "    \n",
    "    Returns: a dataframe object.\n",
    "    \"\"\"\n",
    "    # get reddit submission (request the post)\n",
    "    submission = reddit.submission(url = post_url)\n",
    "    \n",
    "    # only pull top-level comments\n",
    "    submission.comments.replace_more(limit = 0)  \n",
    "    \n",
    "    # extract top-level comments\n",
    "    comments = []\n",
    "    for comment in submission.comments:\n",
    "        if comment.score > 0: # filtering out downvoted content\n",
    "            comments.append({\n",
    "                'body': comment.body,\n",
    "                'upvotes': comment.score,\n",
    "                'top_level': comment.is_root\n",
    "            })\n",
    "    \n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame(comments)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_comments(post_url):\n",
    "    \"\"\"\n",
    "    A function for aggregating ** all ** comment responses to a Reddit post into a pandas dataframe.\n",
    "    \n",
    "    Parameter post_url: the url link to the intended Reddit post. Type string.\n",
    "    \n",
    "    Returns: a dataframe object.\n",
    "    \"\"\"\n",
    "    # get reddit submission (request the post)\n",
    "    submission = reddit.submission(url = post_url)\n",
    "    \n",
    "    comments = []\n",
    "    \n",
    "    submission.comments.replace_more(limit = None) # ensures all comments, not just top-level ones, are pulled\n",
    "    \n",
    "    for comment in submission.comments.list():\n",
    "        if comment.score > 0: # filtering out downvoted content\n",
    "            comments.append({\n",
    "                'body': comment.body,\n",
    "                'upvotes': comment.score,\n",
    "                'top_level': comment.is_root\n",
    "            })\n",
    "\n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame(comments)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then applied these functions to my first question (_What do you think is the best U.S. state?_). I started out only working with top-level comments since these were less time-consuming to request and process, given that there were fewer of them (only 79 out of the total 4105), then later changed to working with all comments to populate the visualization with more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>top_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denial. \\n\\nSeems there are tons of folk in th...</td>\n",
       "      <td>2866</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hawaii feels like a cheat code.</td>\n",
       "      <td>1866</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i live in Colorado, it’s pretty great</td>\n",
       "      <td>1822</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New England is one of the prettiest places on ...</td>\n",
       "      <td>330</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You could spend your whole life traveling in C...</td>\n",
       "      <td>3488</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Us Rhode Islanders just don’t want to tell you...</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Maine. There is a charm to Maine that is unlik...</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Michigan has it's moments.\\n\\nNear freshwater ...</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>I think more context is needed. Best U.S. stat...</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  upvotes  top_level\n",
       "0   Denial. \\n\\nSeems there are tons of folk in th...     2866       True\n",
       "1                     Hawaii feels like a cheat code.     1866       True\n",
       "2               i live in Colorado, it’s pretty great     1822       True\n",
       "3   New England is one of the prettiest places on ...      330       True\n",
       "4   You could spend your whole life traveling in C...     3488       True\n",
       "..                                                ...      ...        ...\n",
       "74  Us Rhode Islanders just don’t want to tell you...        6       True\n",
       "75  Maine. There is a charm to Maine that is unlik...        9       True\n",
       "76  Michigan has it's moments.\\n\\nNear freshwater ...        7       True\n",
       "77                                      New Hampshire       10       True\n",
       "78  I think more context is needed. Best U.S. stat...        8       True\n",
       "\n",
       "[79 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieving top level comments of a post\n",
    "# r/AskReddit: What do you think is the best U.S. state?\n",
    "# https://www.reddit.com/r/AskReddit/comments/172l3yj/what_do_you_think_is_the_best_us_state/\n",
    "df_best_state = get_top_level_comments(\"https://www.reddit.com/r/AskReddit/comments/172l3yj/what_do_you_think_is_the_best_us_state/\")\n",
    "\n",
    "df_best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>top_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denial. \\n\\nSeems there are tons of folk in th...</td>\n",
       "      <td>2871</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hawaii feels like a cheat code.</td>\n",
       "      <td>1857</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i live in Colorado, it’s pretty great</td>\n",
       "      <td>1828</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New England is one of the prettiest places on ...</td>\n",
       "      <td>338</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You could spend your whole life traveling in C...</td>\n",
       "      <td>3490</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>You would need an extra o, for sure.  But dang...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>Haha then why do you have to make up lies to t...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>If you click the link it’s from NY to LA by gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>Name a single lie I said and prove it</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>You're slinging BS from the start  \\n\"wages in...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3426 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  upvotes  top_level\n",
       "0     Denial. \\n\\nSeems there are tons of folk in th...     2871       True\n",
       "1                       Hawaii feels like a cheat code.     1857       True\n",
       "2                 i live in Colorado, it’s pretty great     1828       True\n",
       "3     New England is one of the prettiest places on ...      338       True\n",
       "4     You could spend your whole life traveling in C...     3490       True\n",
       "...                                                 ...      ...        ...\n",
       "3421  You would need an extra o, for sure.  But dang...        1      False\n",
       "3422  Haha then why do you have to make up lies to t...        1      False\n",
       "3423  If you click the link it’s from NY to LA by gr...        1      False\n",
       "3424              Name a single lie I said and prove it        1      False\n",
       "3425  You're slinging BS from the start  \\n\"wages in...        1      False\n",
       "\n",
       "[3426 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieving all comments\n",
    "df_best_state_all = get_all_comments(\"https://www.reddit.com/r/AskReddit/comments/172l3yj/what_do_you_think_is_the_best_us_state/\")\n",
    "\n",
    "df_best_state_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then referenced a [Stackoverflow post](https://stackoverflow.com/questions/59444065/differentiate-between-countries-and-cities-in-spacy-ner) on using the Python library `geonamescache` in conjunction with Spacy for more fine-tuned categorization of geopolitical entities, creating a list of location names for later reference as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code in this block written with assistance of this stackoverflow post: https://stackoverflow.com/questions/59444065/differentiate-between-countries-and-cities-in-spacy-ner\n",
    "# getting lists of country, city, and U.S. state names for filtering later on\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "\n",
    "countries = gc.get_countries()\n",
    "states = gc.get_us_states()\n",
    "cities = gc.get_cities()\n",
    "\n",
    "def gen_dict_extract(var, key):\n",
    "    if isinstance(var, dict):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, (dict, list)):\n",
    "                yield from gen_dict_extract(v, key)\n",
    "    elif isinstance(var, list):\n",
    "        for d in var:\n",
    "            yield from gen_dict_extract(d, key)\n",
    "\n",
    "cities = [*gen_dict_extract(cities, 'name')]\n",
    "states = [*gen_dict_extract(states, 'name')]\n",
    "countries = [*gen_dict_extract(countries, 'name')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Extraction Using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then loaded the `en_core_web_sm` model, later changed to `en_core_web_lg` during the final visualization steps; I also created a list of types of entities that I was interested in including in the interface later on. This list evolved a lot during the course of the project - I initially started out with a much longer list, including organizations, products, and events, but found that relatively few entities of these types were returned from the data. After exploring my visualization, I limited the entity types of investigation to geopolitical entities, general locations, facilities, works of art, and nationalities/religious group, as I observed these types tended to be somewhat more numerous and informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading an nlp object\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# entity types of interest for retaining for visualization later\n",
    "entity_type_list = [\"GPE\", \"LOC\", \"FAC\", \"WORK_OF_ART\", \"NORP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wrote a function for extracting these entities of interest and their corresponding text from a single document. `get_entities` returns a dictionary with a list of texts belonging to each entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_entities(text): \n",
    "    \"\"\"\n",
    "    Helper function for generate_dictionary(df). Given a text, extracts entities into separate lists based on their labels.\n",
    "    \n",
    "    Parameter text: A text of type string.\n",
    "    Returns: a dictionary of lists of entity texts.\n",
    "    \"\"\"\n",
    "    state_entities = set()\n",
    "    geopolitical_entities = []\n",
    "    entity_dict = {entity_type: [] for entity_type in entity_type_list if entity_type != \"GPE\"}\n",
    "        \n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        \n",
    "        if ent.label_ == \"GPE\":\n",
    "            if ent.text.title() in states:\n",
    "                state_entities.add(ent.text)\n",
    "            else:\n",
    "                geopolitical_entities.append(ent.text)\n",
    "        elif ent.label_ in entity_dict:\n",
    "            if ent.label_ != \"GPE\":\n",
    "                entity_dict[ent.label_].append(ent.text)\n",
    "    \n",
    "    result = {\n",
    "        \"states\": list(state_entities),\n",
    "        \"GPE\": list(set(geopolitical_entities)),\n",
    "        **{key: list(set(value)) for key, value in entity_dict.items()}\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, this is what one sample comment from Question 1's top level replies looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love Michigan! 4 seasons the weather's not bad, lots and lots of fresh water! If you like the city, Detroit and the metro area rocks, if you like the outdoors we have huge beautiful forests and more beautiful shoreline than almost any other state in the country! Also I will give a shout out to the Upper Peninsula which is pure God's country!\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one selected comment\n",
    "df_best_state.iloc[15]['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what it looks like after calling `get_entities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'states': ['Michigan'],\n",
       " 'GPE': ['Detroit'],\n",
       " 'LOC': ['the Upper Peninsula'],\n",
       " 'FAC': [],\n",
       " 'WORK_OF_ART': [],\n",
       " 'NORP': []}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of output of calling get_entities on the previous sample comment\n",
    "get_entities(df_best_state.iloc[15]['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function as a helper, I then eventually created another function called `add_entities_to_df` that takes a dataframe of comments and adds columns to the right, essentially one column for each entity type. Each column added consists of a list of strings. The final form of this function required some experimentation to limit redundancy and to ensure the added entities aligned properly by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_entities_to_df(df):\n",
    "    \"\"\"\n",
    "    Function that adds entity columns to a Reddit comment dataframe where each row is one comment.\n",
    "    \n",
    "    Parameter df: a dataframe holding comment data for a Reddit post where each row is one comment. \n",
    "    Returns: the input dataframe with extra columns, one for each entity type of interest, which contain lists of corresponding entity texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializes empty dictionary\n",
    "    location_data = {}\n",
    "    \n",
    "    # calls first helper function get_entities, which returns - for each comment - a dictionary of lists of entities separated by type\n",
    "    entities_lists = df[\"body\"].apply(get_entities)\n",
    "    \n",
    "    # turns the returned dictionary into a dataframe where each entity type is its own column\n",
    "    entities_df = pd.json_normalize(entities_lists)\n",
    "    \n",
    "    # sticking this dataframe to the right of the original dataframe so each comment now has lists of entities separated into different columns by type in their row\n",
    "    df_merged = pd.concat([df, entities_df], axis = 1)\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can take a look at the first row of the Reddit data before calling `add_entities_to_df`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>top_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denial. \\n\\nSeems there are tons of folk in th...</td>\n",
       "      <td>2866</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  upvotes  top_level\n",
       "0  Denial. \\n\\nSeems there are tons of folk in th...     2866       True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best_state.iloc[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... And after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>top_level</th>\n",
       "      <th>states</th>\n",
       "      <th>GPE</th>\n",
       "      <th>LOC</th>\n",
       "      <th>FAC</th>\n",
       "      <th>WORK_OF_ART</th>\n",
       "      <th>NORP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denial. \\n\\nSeems there are tons of folk in th...</td>\n",
       "      <td>2866</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  upvotes  top_level  \\\n",
       "0  Denial. \\n\\nSeems there are tons of folk in th...     2866       True   \n",
       "\n",
       "  states GPE LOC FAC WORK_OF_ART NORP  \n",
       "0     []  []  []  []          []   []  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_entities_to_df(df_best_state.iloc[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most scripting-intensive part of the project occurred at this point. In order to get the data into a form that can be easily visualized in a web map, the data needs to transform from being organized by comment to being organized by state, ideally in a JSON-serializable dictionary format. In order to do this, I wrote two more functions - df-wide `generate_dictionary` and row-wide helper `reformat_data` - in order to transform my data from a collection of Reddit comments discussing states alongside other entities to a dictionary where each state is a key with the following properties:\n",
    "\n",
    "- `weight` - the cumulative number of upvotes received by comments that mentioned this state,\n",
    "- `comments` - a list of strings, each the full text of a top-level comment mentioning this state,\n",
    "- `replies` - a list of strings, each the full text of a non-top-level comment mentioning this state, and \n",
    "- entity types, each representing a list of strings (entity texts of that type mentioned in the same comment as the state of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dictionary(df):\n",
    "    \"\"\"\n",
    "    Function that takes a Reddit comment dataframe with no added information as input and ultimately returns a \n",
    "    JSON-serializable dictionary indexed by U.S. state name.\n",
    "    \n",
    "    Parameter df: a Reddit comment dataframe with columns `body`, `upvotes`, and `top_level`, where `body` refers to the comment \n",
    "    text, `upvotes` refers to the number of upvotes on the comment, and `top_level` is a boolean indicating if the comment is a \n",
    "    direct reply to the post or not.\n",
    "    \n",
    "    Returns location_data: a dictionary of U.S. state names, each one representing a key to the following properties: \n",
    "    `weight` - the cumulative number of upvotes received by comments that mentioned this state,\n",
    "    `comments` - a list of strings, each the full text of a top-level comment mentioning this state,\n",
    "    `replies` - a list of strings, each the full text of a non-top-level comment mentioning this state,\n",
    "    and a series of entity type strings, each representing a list of strings (entity texts of that type mentioned in the same \n",
    "    comment as the state of interest).\n",
    "    \n",
    "    \"\"\"\n",
    "     # initializes empty dictionary\n",
    "    location_data = {}\n",
    "    \n",
    "    # calling add_entities_to_df to add entity columns to each comment\n",
    "    df_merged = add_entities_to_df(df)\n",
    "                                   \n",
    "    # calling helper function reformat_data, which populates the previously initiated empty dictionary, with data by US state\n",
    "    df_merged.apply(lambda row: reformat_data(row, location_data), axis = 1)\n",
    "    \n",
    "    return location_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reformat_data(df_row, location_data):\n",
    "    \"\"\"\n",
    "    Helper function for generate_dictionary(df) that takes a row of a dataframe and updates a dictionary holding information for an overall dataframe. \n",
    "    Generates a dictionary whose keys are U.S. state names and their aggregated information instead of being indexed by comment as in the dataframe.\n",
    "    \n",
    "    Parameter df_row: a row of a dataframe of Reddit comments. Includes a comment's body text, upvote count, and entities (separated by type).\n",
    "    Parameter location_data: a dictionary where each U.S. state is a key and has the following properties: \n",
    "    \"\"\"\n",
    "    # list of states mentioned in this row (aka in this one comment)\n",
    "    states_list = [state for state in df_row[\"states\"]]\n",
    "    \n",
    "    # iterating over each state mentioned in this comment\n",
    "    for state in states_list:\n",
    "        \n",
    "        # if this state hasn't already been added to the dictionary, initialize an entry for it\n",
    "        if state not in location_data:\n",
    "            location_data[state] = {}\n",
    "            location_data[state][\"weight\"] = df_row[\"upvotes\"] # add a key for upvote info\n",
    "            \n",
    "            if df_row[\"top_level\"]: # if this comment is top level, add it to a key for storing comments\n",
    "                location_data[state][\"comments\"] = [df_row[\"body\"]]\n",
    "                location_data[state][\"replies\"] = [] # initiate key for storing replies, empty for now\n",
    "        \n",
    "            else: # otherwise, add it to a key for storing replies\n",
    "                location_data[state][\"comments\"] = []\n",
    "                location_data[state][\"replies\"] = [df_row[\"body\"]]\n",
    "                \n",
    "            \n",
    "            # iterating over entity types of interest, creating a key for each state that holds a list of entities\n",
    "            for entity_type in entity_type_list:\n",
    "                location_data[state][entity_type] = df_row[entity_type]\n",
    "                \n",
    "        else:\n",
    "            # the state has already been added to the dictionary - access the entry and update the fields\n",
    "            location_data[state][\"weight\"] += df_row[\"upvotes\"] # add this comment's upvotes to this state's `weight` key\n",
    "            \n",
    "            if df_row[\"top_level\"]:\n",
    "                location_data[state][\"comments\"].append(df_row[\"body\"]) # add this comment's text to the list of comments for this state\n",
    "            else:\n",
    "                location_data[state][\"replies\"].append(df_row[\"body\"])\n",
    "            \n",
    "            # iterating over entity types of interest, creating a key for each state that holds a list of entities\n",
    "            for entity_type in entity_type_list:\n",
    "                if df_row[entity_type]: \n",
    "                    location_data[state][entity_type].extend(df_row[entity_type])\n",
    "                    location_data[state][entity_type] = list(set(location_data[state][entity_type]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate what effect `generate_dictionary` has, we can look at the first 10 comments before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>top_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denial. \\n\\nSeems there are tons of folk in th...</td>\n",
       "      <td>2866</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hawaii feels like a cheat code.</td>\n",
       "      <td>1866</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i live in Colorado, it’s pretty great</td>\n",
       "      <td>1822</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New England is one of the prettiest places on ...</td>\n",
       "      <td>330</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You could spend your whole life traveling in C...</td>\n",
       "      <td>3488</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I’ve been to all 50 and my personal favorite t...</td>\n",
       "      <td>198</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Washington and NY are my favourites. West coas...</td>\n",
       "      <td>180</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oregon 300 miles of public beaches</td>\n",
       "      <td>432</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>If money isn't an issue, California.</td>\n",
       "      <td>537</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Washington hands down. I lived there briefly a...</td>\n",
       "      <td>616</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  upvotes  top_level\n",
       "0  Denial. \\n\\nSeems there are tons of folk in th...     2866       True\n",
       "1                    Hawaii feels like a cheat code.     1866       True\n",
       "2              i live in Colorado, it’s pretty great     1822       True\n",
       "3  New England is one of the prettiest places on ...      330       True\n",
       "4  You could spend your whole life traveling in C...     3488       True\n",
       "5  I’ve been to all 50 and my personal favorite t...      198       True\n",
       "6  Washington and NY are my favourites. West coas...      180       True\n",
       "7                 Oregon 300 miles of public beaches      432       True\n",
       "8               If money isn't an issue, California.      537       True\n",
       "9  Washington hands down. I lived there briefly a...      616       True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best_state.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hawaii': {'weight': 1866,\n",
       "  'comments': ['Hawaii feels like a cheat code.'],\n",
       "  'replies': [],\n",
       "  'GPE': [],\n",
       "  'LOC': [],\n",
       "  'FAC': [],\n",
       "  'WORK_OF_ART': [],\n",
       "  'NORP': []},\n",
       " 'Colorado': {'weight': 1822,\n",
       "  'comments': ['i live in Colorado, it’s pretty great'],\n",
       "  'replies': [],\n",
       "  'GPE': [],\n",
       "  'LOC': [],\n",
       "  'FAC': [],\n",
       "  'WORK_OF_ART': [],\n",
       "  'NORP': []},\n",
       " 'California': {'weight': 4025,\n",
       "  'comments': ['You could spend your whole life traveling in California and never see the whole thing or get tired of the landscape.',\n",
       "   \"If money isn't an issue, California.\"],\n",
       "  'replies': [],\n",
       "  'GPE': [],\n",
       "  'LOC': [],\n",
       "  'FAC': [],\n",
       "  'WORK_OF_ART': [],\n",
       "  'NORP': []},\n",
       " 'Maine': {'weight': 198,\n",
       "  'comments': ['I’ve been to all 50 and my personal favorite to visit has been Maine!'],\n",
       "  'replies': [],\n",
       "  'GPE': [],\n",
       "  'LOC': [],\n",
       "  'FAC': [],\n",
       "  'WORK_OF_ART': [],\n",
       "  'NORP': []},\n",
       " 'Washington': {'weight': 796,\n",
       "  'comments': ['Washington and NY are my favourites. West coast and east coast, both beautiful places but very different at the same time.',\n",
       "   'Washington hands down. I lived there briefly and I miss it every day.\\n\\nUnfortunately I am in Florida, man. For some reason people want to live here, idk why.'],\n",
       "  'replies': [],\n",
       "  'GPE': ['NY'],\n",
       "  'LOC': ['West coast', 'east coast'],\n",
       "  'FAC': [],\n",
       "  'WORK_OF_ART': [],\n",
       "  'NORP': []},\n",
       " 'Oregon': {'weight': 432,\n",
       "  'comments': ['Oregon 300 miles of public beaches'],\n",
       "  'replies': [],\n",
       "  'GPE': [],\n",
       "  'LOC': [],\n",
       "  'FAC': [],\n",
       "  'WORK_OF_ART': [],\n",
       "  'NORP': []},\n",
       " 'Florida': {'weight': 616,\n",
       "  'comments': ['Washington hands down. I lived there briefly and I miss it every day.\\n\\nUnfortunately I am in Florida, man. For some reason people want to live here, idk why.'],\n",
       "  'replies': [],\n",
       "  'GPE': [],\n",
       "  'LOC': [],\n",
       "  'FAC': [],\n",
       "  'WORK_OF_ART': [],\n",
       "  'NORP': []}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dictionary(df_best_state.iloc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iterating over my functions and the intended output format, I finally reached the point where the comment data for the first question could be converted into a `.json` file - a standard format for interactive web data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generating a dictionary of US states where for each one, we have collected comments that mentioned them and associated entities of particular types\n",
    "dictionary_best_state_all = generate_dictionary(df_best_state_all)\n",
    "\n",
    "# saving this dictionary as a json file for visualization in javascript\n",
    "file_path = \"best-state-results-all.json\"\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(dictionary_best_state_all, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What did you find? How did you find it? How should we read your figures? Be sure to include confidence intervals or other measures of statistical significance or uncetainty where appropriate._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After producing a `.json` file containing the Reddit comments and spaCy entities associated with each U.S. state, I followed a fairly standard Javascript workflow (loading the file using d3.js and constructing a U.S. map using a separate topojson file). Further interactions such as populating a sidebar with state data upon hover and click were also implemented using Javascript. The final interface appears as below and is accessible live at [https://tammyzhang-1.github.io/reddit-text-viz/](https://tammyzhang-1.github.io/reddit-text-viz/) (best viewed on a Macbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Home Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A screenshot of the visualization upon launch](home.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States were colored based on their `weight`, calculated through the functions presented above as the aggregated count of upvotes across all comments that mentioned that state, with states of higher weight having darker fills than states with lower weight. However, this should be regarded not as a presentation of \"what r/AskReddit users think is the best state\", but instead \"what states tend to be discussed most often talking about what is the best state\". The distinction is subtle but crucial - hovering over and exploring individual comments tagged for each state reveal a good number of sarcastic or negative replies (such as \"definitely not Illinois\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing State Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sidebar populated with state information](sidebar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon clicking or hovering on a state, the sidebar populates with a scrollable box of all comments mentioning this state, as well as lists of entities mentioned in the same content. While some entities appear to be rather unrelated to or distant from the state at hand - likely due to unresolved co-references in the same comment to other locations - there are some notable patterns of co-occurrences that make sense - such as the appearance of major national parks in the LOC section, suggesting that access to nature may be a theme in the discussion of what makes a state \"the best\". Similarly, the mention of particular groups (NORPs) such as different racial groups, religious groups, and political parties (notably Christians and republicans in southern states) also suggest the presence of a related dialogue existing in the discussion centered around one's attitude towards these groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switching Between Questions / Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dropdown with question options expanded](dropdown.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After verifying that the `.json` file output by my script could viably be visualized using Javascript, I replicated the process for the four other questions of interest. This was a surprisingly quick process - after the script to request each post's comments had run, only a few lines of Python were needed in order to repeat processing the data. On the web application side, it took less than a minute to add each additional post's map once the file was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_state_data_from_reddit(url, output_file):\n",
    "    \"\"\"\n",
    "    Making use of the previously defined functions to create a function that takes a url to a Reddit post, searches the top level\n",
    "    comments for entities associated with U.S. states using spacy, and creates a json file that can be visualized in a web \n",
    "    browser using a simplistic Javascript map.\n",
    "    \n",
    "    Returns: none. Writes a json file to the current directory with name output_file.\n",
    "    Parameter url: String. A URL to a Reddit post that might have comments pertaining to U.S. states.\n",
    "    Parameter output_file: String. A name for the output json file generated from the comments, containing information aggregated by U.S. state.\n",
    "    \"\"\"\n",
    "    df = get_all_comments(url)\n",
    "    state_dict = generate_dictionary(df)\n",
    "    \n",
    "    file_path = output_file\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(state_dict, json_file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not rerun in order to avoid exceeding Reddit rate limits\n",
    "get_state_data_from_reddit(\"https://www.reddit.com/r/AskReddit/comments/13mat7e/what_is_perhaps_the_least_talked_about_us_state/\", \"least-talked-about.json\")\n",
    "get_state_data_from_reddit(\"https://www.reddit.com/r/AskReddit/comments/1bfq75y/whats_the_one_us_state_you_absolutely_will_never/\", \"avoided-states.json\")\n",
    "get_state_data_from_reddit(\"https://www.reddit.com/r/AskReddit/comments/4gjd4h/us_redditors_what_does_your_state_do_better_than/\", \"better-states.json\")\n",
    "get_state_data_from_reddit(\"https://www.reddit.com/r/AskReddit/comments/3uahca/all_50_states_are_getting_together_for/\", \"thanksgiving.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maps aligned fairly well visually with the conclusions that one might get from quickly checking top comments at a glance, or from general intuition about the U.S. landscape. For example, when asked what is perhaps the least talked about US state, discussion visibly shifted towards more rural states such as the Dakotas, Montana, Wyoming, etc (shown through darker colors on the map). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the datasets through the map yielded some interesting insights - it was certainly a new way of viewing the information once contained in nested layers of comments underneath a post. By organizing comments spatially, it was easier to quickly gather ideas about shared experiences and patterns across the country. For example, the question on what state you would never step foot in and why was marked by increased references to particular entities - especially location-based ones, including remote roads and towns that might be associated with crimes or folklore - and increased discussion of racial groups. A quick overview of comments regarding southern states confirmed that much of one's avoidance of particular states could be motivated by safety concerns related to one's race/ethnicity in historically dangerous areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What does it all mean? Do your results support your hypothesis? Why or why not? What are the limitations of your study and how might those limitations be addressed in future work?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, results were consistent with my hypothesis. Utilizing very basic text processing methods such as spaCy's entity recognition allowed the extraction of more data to consider when attempting to visualize responses to r/AskReddit questions, and could be rapidly extended in the creation of similar data visualizations for a topic. However, limitations are considerable, mostly in part to the simplicity of the method included, and many exciting directions for future work in this area exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work was marked by major limitations through its usage of an extremely simple approach - aggregating comments based on their mention of U.S. states and associating each state with different spaCy-recognized entities for display alongside a geospatial visualization. As mentioned earlier, the lack of further processing means that we cannot distinguish if a state is mentioned as an intended answer to a question or as part of an expression of disagreement/opposition. Furthermore, many comments do not only mention one state - some comments were very lengthy and included a large range of states and entities. Under this current workflow, all entities would be associated with all states mentioned in the same content. A more developed, refined, and customized NLP pipeline is needed to isolate particular targets from one another and pick out which entities are associated with which states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While exploring the visualizations produced from the dataset, I also felt that while there certainly were emergent patterns made more visible through the inclusion of spaCy entities, the approach of just listing all entities out as text is limited. Including entities that may not make sense - such as locations very far away from a state, or text that appears to be miscategorized / not semantically meaningful - can serve to distract the user. While this project was helpful in confirming that it is technically feasible to collect types of text related to specific objects in a crowd question-and-answer instance, reindex this data to look at it from another angle, and then display this information in the browser, it is still important for data visualization designers to understand what relationships they want to make clearly visible in their graphics beforehand, and then tailor their workflows to produce clean, relevant datasets. Future work may involve customizing spaCy workflows to possibly identify the most relevant entities to a visualization, as well as experimentation with ways of visualizing these entities themselves. Alternatively, this kind of work with outputting aggregated spaCy entities could have applications for scientists who want to interpret the results of their models.\n",
    "\n",
    "Finally, it should be noted that any work with Reddit involves working with user-generated content, which come with their own set of special ethical concerns. While I did not collect or keep any associated user information with each comment, I did retain individual comment texts in this project for use in visualization so that they could be referred back to later on. It is possible that exploring alternative ways of visualizing Reddit comments may result in people having their responses - which they might have assumed to be fairly hidden or buried under other comments - becoming more visible than they would have expected, and particular care needs to be extended when handling data that may include highly personal experiences or narratives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying a very simplistic base experiment with adding in computationally processed text data via spaCy entity recognition, I ultimately have the sense that there are many possible future directions for including NLP methods in interactive data visualization development. Getting the data into the browser, as done in this project, is the first step - what can be done with the data afterwards, and what unique or insightful new forms it can take, is all but limitless.\n",
    "\n",
    "An interesting idea is the further automation of generating visualizations for comments under Reddit posts - not just in the form of U.S. state maps or even world maps for location-related topics, but in a even more diverse array of forms. For example, visualizations related to clustering or network diagrams, commonly used for visualizations on a broader scale but which might have interesting results even when applied to the comments under one post - particularly posts from communities with high engagement and concentration around particular topics at a time, such as questions posed on r/AskReddit. \n",
    "\n",
    "This project showed that it can be possible to write one Python script that can be used on multiple post-comment collections to generate web visualizations for similar posts quickly. Developing webpages with a Python server that can run text-processing functions, or the script shown here, means that visualizations can be generated on demand by users. For example, a user could be able to paste in a link to a Reddit post of interest, wait for a script in the background to generate a visualization, and proceed to explore from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another component I sketched in early iterations of the interface for this project visualization was the inclusion of a [radar chart](https://en.wikipedia.org/wiki/Radar_chart) or something similar for visualizing word embedding cosine similarity with particular vectors representing concepts of interest (see Nelson, 2021). For example, we could identify areas of interest/concern with questions such as \"What do you think is the best state in the U.S.?\", possibly construct vectors related to these concerns (summarized as \"nature\", \"affordability\", \"safety\", etc), and visualize how much each state's discussion is associated with each area through a radar chart where each point represents the concept. While this was not ultimately included in this project due to feasibility constraints, it represents only one of a mulititude of ways I imagine text processing methods can be used to support data visualization in future projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
